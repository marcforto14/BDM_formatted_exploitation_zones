{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"Preprocessing BDM\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000) \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idealista Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union Files in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, to_date\n",
    "\n",
    "idealista_data_path = '../P2_data/idealista'\n",
    "\n",
    "files = [file for file in os.listdir(idealista_data_path) if file.endswith('_idealista')]\n",
    "necessary_columns = {'bathrooms', 'date', 'district', 'exterior', 'floor', 'latitude', 'longitude', 'newDevelopment', \n",
    "                     'price', 'propertyCode', 'propertyType', 'rooms', 'size', 'status'}\n",
    "dfs_idealista = []\n",
    "\n",
    "# Function to read, flatten, and add date column to DataFrames\n",
    "def read(file):\n",
    "    date_str = file.split('_idealista')[0]\n",
    "    date = to_date(lit(date_str), 'yyyy_MM_dd')\n",
    "    file_path = os.path.join(idealista_data_path, file)\n",
    "    try:\n",
    "        df = spark.read.parquet(file_path)\n",
    "        df = df.withColumn('date', date)\n",
    "        df = df.select([col for col in necessary_columns if col in df.columns])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_idealista = [read(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union all DataFrames\n",
    "union_dfs_idealista = dfs_idealista[0]\n",
    "for df in dfs_idealista[1:]:\n",
    "    union_dfs_idealista = union_dfs_idealista.unionByName(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle duplicates, reconciliate data, clean, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate based on propertyCode, keeping the latest data\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "# Repartition to avoid memory issues\n",
    "num_partitions = 200  # Adjust based on your cluster configuration and data size\n",
    "union_dfs_idealista = union_dfs_idealista.repartition(num_partitions)\n",
    "\n",
    "# Find the latest date for each propertyCode\n",
    "latest_dates = union_dfs_idealista.groupBy(\"propertyCode\").agg(spark_max(\"date\").alias(\"latestDate\"))\n",
    "\n",
    "# Perform a left semi join to filter the union_dfs_idealista based on the latest dates\n",
    "latest_dfs_idealista = union_dfs_idealista.join(\n",
    "    latest_dates,\n",
    "    (union_dfs_idealista.propertyCode == latest_dates.propertyCode) & \n",
    "    (union_dfs_idealista.date == latest_dates.latestDate),\n",
    "    \"left_semi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenData Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f'../P2_data/income_opendata/income_opendata_neighborhood.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"info_last\",(col(\"info\")[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"info_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"population_district\",col(\"info_last.pop\"))\n",
    "df = df.withColumn(\"rfd_district\",\"info_last.rfd\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
