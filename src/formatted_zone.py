# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: -all
#     formats: py,ipynb
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.16.2
#   kernelspec:
#     display_name: base
#     language: python
#     name: python3
# ---

from pyspark.sql import SparkSession
import os
from pyspark.sql.functions import col, lit, to_date

from pyspark.sql import functions as F
from pyspark.sql.functions import max as spark_max

VM_IP = "10.4.41.51"

spark = (
    SparkSession.builder 
    .appName("Preprocessing BDM")
    .config("spark.executor.memory", "4g")
    .config("spark.driver.memory", "2g")
    .config("spark.sql.debug.maxToStringFields", 1000) 
    .config('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \
    .getOrCreate()
)

# # Preprocessing

# ## Idealista Dataset

# ### Union Files in one dataframe

# +

idealista_data_path = '../P2_data/idealista'

files = [file for file in os.listdir(idealista_data_path) if file.endswith('_idealista')]
necessary_columns = {'bathrooms', 'date', 'district', 'exterior', 'floor', 'latitude', 'longitude', 'newDevelopment', 
                     'price', 'propertyCode', 'propertyType', 'rooms', 'size', 'status', 'neighborhood'}
dfs_idealista = []

# Function to read, flatten, and add date column to DataFrames
def read(file):
    date_str = file.split('_idealista')[0]
    date = to_date(lit(date_str), 'yyyy_MM_dd')
    file_path = os.path.join(idealista_data_path, file)
    try:
        df = spark.read.parquet(file_path)
        df = df.withColumn('date', date)
        df = df.select([col for col in necessary_columns if col in df.columns])
        return df
    except Exception as e:
        print(f"Error reading {file_path}: {str(e)}")

dfs_idealista = [read(file) for file in files]

# Union all DataFrames
union_dfs_idealista = dfs_idealista[0]
for df in dfs_idealista[1:]:
    if 'neighborhood' not in df.columns:
        df = df.withColumn("neighborhood", lit(None).cast("string"))
    union_dfs_idealista = union_dfs_idealista.unionByName(df)

union_dfs_idealista.repartition(16, [col("propertyCode"), col("date")]).write.mode("overwrite").parquet(f"../temporal_zone/acumulated_idealista")
union_dfs_idealista = spark.read.parquet(f"../temporal_zone/acumulated_idealista")

# ### Handle duplicates, reconciliate data, clean, etc.
# Deduplicate based on propertyCode, keeping the latest data
# Find the latest date for each propertyCode
latest_dates = union_dfs_idealista.groupBy("propertyCode").agg(spark_max("date").alias("latestDate"))

# Perform a left semi join to filter the union_dfs_idealista based on the latest dates
latest_dfs_idealista = union_dfs_idealista.join(
    latest_dates,
    (union_dfs_idealista.propertyCode == latest_dates.propertyCode) & 
    (union_dfs_idealista.date == latest_dates.latestDate),
    "left_semi"
)
# -

latest_dfs_idealista.repartition(8, col("neighborhood")).write.mode("overwrite").parquet("../temporal_zone/idealista")


# ## OpenData Dataset
df = spark.read.format("mongo").option('uri', f"mongodb://{VM_IP}/idealista_collection.income_open_data").load()

df = df.withColumnRenamed("neigh_name ", "neigh_name")

exploded_df = df.select("_id", "district_id", "neigh_name", "district_name", F.explode("info").alias("value_per_year"))
income_flattened = exploded_df.select("_id", "district_id", "neigh_name", "district_name", col("value_per_year.RFD").alias("rfd"), col("value_per_year.pop").alias("population"), col("value_per_year.year").alias("year"))

income_flattened.write.mode("overwrite").parquet("../temporal_zone/neighborhood_data")


# ## Look-up table

# +
income_flattened = spark.read.parquet("../temporal_zone/neighborhood_data")
latest_dfs_idealista = spark.read.parquet("../temporal_zone/idealista")

lookup_ne_income = (
    spark.read.format("mongo").option('uri', f"mongodb://{VM_IP}/idealista_collection.lookup_neighorhood_income").load()
    .select("neighborhood", col("_id").alias("id_neighorhood"))
)
lookup_dist_income = (
    spark.read.format("mongo").option('uri', f"mongodb://{VM_IP}/idealista_collection.lookup_distict_income").load()
    .select("district", col("_id").alias("id_district"))
)

lookup_ne_idealista = (
    spark.read.format("mongo").option('uri', f"mongodb://{VM_IP}/idealista_collection.lookup_rent_neighorhood").load()
    .select("ne", col("_id").alias("id_neighorhood"))
)

lookup_dist_idealista = (
    spark.read.format("mongo").option('uri', f"mongodb://{VM_IP}/idealista_collection.lookup_rent_district").load()
    .select("di", col("_id").alias("id_district"))
)

# -

income_flattened = income_flattened.join(F.broadcast(lookup_ne_income), on=income_flattened['neigh_name'] == lookup_ne_income['neighborhood'])
income_flattened = income_flattened.join(F.broadcast(lookup_dist_income), on=income_flattened['district_name'] == lookup_dist_income['district'])


latest_dfs_idealista = latest_dfs_idealista.join(F.broadcast(lookup_ne_idealista), on=latest_dfs_idealista['neighborhood'] == lookup_ne_idealista['ne'])
latest_dfs_idealista = latest_dfs_idealista.join(F.broadcast(lookup_dist_idealista), on=latest_dfs_idealista['district'] == lookup_dist_idealista['di'])


income_flattened.write.mode("overwrite").parquet("../exploitation_zone/neighborhood_data")
latest_dfs_idealista.write.mode("overwrite").parquet("../exploitation_zone/idealista")

# ## Extra Dataset

rdd = spark.read.json("../P2_data/unemployment-data").select((col("result.records"))).rdd

# +
from pyspark.sql.types import StructType, StructField, StringType, LongType, FloatType, IntegerType
# Step 3: Flatten the RDD and extract the records
flattened_rdd = rdd.flatMap(lambda x: x["records"])

# Step 4: Define the schema
schema = StructType([
    StructField("Any", StringType(), True),
    StructField("Codi_Barri", StringType(), True),
    StructField("Codi_Districte", StringType(), True),
    StructField("Mes", StringType(), True),
    StructField("Nom_Barri", StringType(), True),
    StructField("Nom_Districte", StringType(), True),
    StructField("Pes_atur", StringType(), True),
    StructField("Poblacio_16_64_anys", StringType(), True),
    StructField("_id", LongType(), True)
])

# Step 5: Convert RDD to DataFrame using the schema
unemployment_df = spark.createDataFrame(flattened_rdd, schema)
# -

unemployment_df = unemployment_df.join(F.broadcast(lookup_ne_income), on=unemployment_df['Nom_Barri'] == lookup_ne_income['neighborhood'], how="left")
unemployment_df = unemployment_df.join(F.broadcast(lookup_dist_income), on=unemployment_df['Nom_Districte'] == lookup_dist_income['district'])

unemployment_selected = unemployment_df.select("Any", "Mes", "Pes_atur", "Poblacio_16_64_anys", "id_neighorhood", "id_district")
unemployment_selected = unemployment_selected.withColumn("Any", col("any").cast("int"))
unemployment_selected = unemployment_selected.withColumn("mes", col("mes").cast("int"))
unemployment_selected = unemployment_selected.withColumn("Pes_atur", col("Pes_atur").cast("float"))
unemployment_selected = unemployment_selected.withColumn("Poblacio_16_64_anys", col("Poblacio_16_64_anys").cast("int"))


unemployment_selected.write.mode("overwrite").parquet("../exploitation_zone/unemployment_data")

unemployment_selected




