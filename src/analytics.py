from pyspark.sql import SparkSession
import os
from pyspark.sql.functions import col, lit, to_date

from pyspark.sql import functions as F
from pyspark.sql.functions import max as spark_max

spark = (
    SparkSession.builder 
    .appName("Preprocessing BDM")
    .config("spark.executor.memory", "4g")
    .config("spark.driver.memory", "2g")
    .config("spark.sql.debug.maxToStringFields", 1000) 
    .getOrCreate()
)

# # Preprocessing

# ## Idealista Dataset

# ### Union Files in one dataframe

# +

idealista_data_path = '../P2_data/idealista'

files = [file for file in os.listdir(idealista_data_path) if file.endswith('_idealista')]
necessary_columns = {'bathrooms', 'date', 'district', 'exterior', 'floor', 'latitude', 'longitude', 'newDevelopment', 
                     'price', 'propertyCode', 'propertyType', 'rooms', 'size', 'status', 'neighborhood'}
dfs_idealista = []

# Function to read, flatten, and add date column to DataFrames
def read(file):
    date_str = file.split('_idealista')[0]
    date = to_date(lit(date_str), 'yyyy_MM_dd')
    file_path = os.path.join(idealista_data_path, file)
    try:
        df = spark.read.parquet(file_path)
        df = df.withColumn('date', date)
        df = df.select([col for col in necessary_columns if col in df.columns])
        return df
    except Exception as e:
        print(f"Error reading {file_path}: {str(e)}")


# -

dfs_idealista = [read(file) for file in files]

# Union all DataFrames
union_dfs_idealista = dfs_idealista[0]
for df in dfs_idealista[1:]:
    if 'neighborhood' not in df.columns:
        df = df.withColumn("neighborhood", lit(None).cast("string"))
    union_dfs_idealista = union_dfs_idealista.unionByName(df)

union_dfs_idealista.repartition(16, [col("propertyCode"), col("date")]).write.mode("overwrite").parquet(f"../temporal_zone/acumulated_idealista")
union_dfs_idealista = spark.read.parquet(f"../temporal_zone/acumulated_idealista")

# ### Handle duplicates, reconciliate data, clean, etc.

# +
# Deduplicate based on propertyCode, keeping the latest data

# Find the latest date for each propertyCode
latest_dates = union_dfs_idealista.groupBy("propertyCode").agg(spark_max("date").alias("latestDate"))

# Perform a left semi join to filter the union_dfs_idealista based on the latest dates
latest_dfs_idealista = union_dfs_idealista.join(
    latest_dates,
    (union_dfs_idealista.propertyCode == latest_dates.propertyCode) & 
    (union_dfs_idealista.date == latest_dates.latestDate),
    "left_semi"
)
# -

latest_dfs_idealista.repartition(8, col("neighborhood")).write.mode("overwrite").parquet("../temporal_zone/idealista")


# ## OpenData Dataset

df = spark.read.json(f'../P2_data/income_opendata/income_opendata_neighborhood.json')

df = df.withColumnRenamed("neigh_name ", "neigh_name")

exploded_df = df.select("_id", "district_id", "neigh_name", "district_name", F.explode("info").alias("value_per_year"))
income_flattened = exploded_df.select("_id", "district_id", "neigh_name", "district_name", col("value_per_year.RFD").alias("rfd"), col("value_per_year.pop").alias("population"), col("value_per_year.year").alias("year"))

income_flattened.write.mode("overwrite").parquet("../temporal_zone/neighborhood_data")


# ## Look-up table

# +
income_flattened = spark.read.parquet("../temporal_zone/neighborhood_data")
latest_dfs_idealista = spark.read.parquet("../temporal_zone/idealista")

lookup_ne_income = spark.read.json("../P2_data/lookup_tables/income_lookup_neighborhood.json").select("neighborhood", col("_id").alias("id_neighorhood"))
lookup_dist_income = spark.read.json("../P2_data/lookup_tables/income_lookup_district.json").select("district", col("_id").alias("id_district"))

lookup_ne_idealista = spark.read.json("../P2_data/lookup_tables/rent_lookup_neighborhood.json").select("ne", col("_id").alias("id_neighorhood"))
lookup_dist_idealista = spark.read.json("../P2_data/lookup_tables/rent_lookup_district.json").select("di", col("_id").alias("id_district"))

# -

income_flattened = income_flattened.join(F.broadcast(lookup_ne_income), on=income_flattened['neigh_name'] == lookup_ne_income['neighborhood'])
income_flattened = income_flattened.join(F.broadcast(lookup_dist_income), on=income_flattened['district_name'] == lookup_dist_income['district'])


latest_dfs_idealista = latest_dfs_idealista.join(F.broadcast(lookup_ne_idealista), on=latest_dfs_idealista['neighborhood'] == lookup_ne_idealista['ne'])
latest_dfs_idealista = latest_dfs_idealista.join(F.broadcast(lookup_dist_idealista), on=latest_dfs_idealista['district'] == lookup_dist_idealista['di'])


income_flattened.write.mode("overwrite").parquet("../exploitation_zone/neighborhood_data")
latest_dfs_idealista.write.mode("overwrite").parquet("../exploitation_zone/idealista")
