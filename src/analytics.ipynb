{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder \n",
    "    .appName(\"Preprocessing BDM\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", 1000) \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idealista Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union Files in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, to_date\n",
    "\n",
    "idealista_data_path = '../P2_data/idealista'\n",
    "\n",
    "files = [file for file in os.listdir(idealista_data_path) if file.endswith('_idealista')]\n",
    "necessary_columns = {'bathrooms', 'date', 'district', 'exterior', 'floor', 'latitude', 'longitude', 'newDevelopment', \n",
    "                     'price', 'propertyCode', 'propertyType', 'rooms', 'size', 'status', 'neighborhood'}\n",
    "dfs_idealista = []\n",
    "\n",
    "# Function to read, flatten, and add date column to DataFrames\n",
    "def read(file):\n",
    "    date_str = file.split('_idealista')[0]\n",
    "    date = to_date(lit(date_str), 'yyyy_MM_dd')\n",
    "    file_path = os.path.join(idealista_data_path, file)\n",
    "    try:\n",
    "        df = spark.read.parquet(file_path)\n",
    "        df = df.withColumn('date', date)\n",
    "        df = df.select([col for col in necessary_columns if col in df.columns])\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_idealista = [read(file) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union all DataFrames\n",
    "union_dfs_idealista = dfs_idealista[0]\n",
    "for df in dfs_idealista[1:]:\n",
    "    if 'neighborhood' not in df.columns:\n",
    "        df = df.withColumn(\"neighborhood\", lit(None).cast(\"string\"))\n",
    "    union_dfs_idealista = union_dfs_idealista.unionByName(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/14 19:53:48 WARN DAGScheduler: Broadcasting large task binary with size 1592.4 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "union_dfs_idealista.repartition(16, [col(\"propertyCode\"), col(\"date\")]).write.mode(\"overwrite\").parquet(f\"../temporal_zone/acumulated_idealista\")\n",
    "union_dfs_idealista = spark.read.parquet(f\"../temporal_zone/acumulated_idealista\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle duplicates, reconciliate data, clean, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate based on propertyCode, keeping the latest data\n",
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "# Find the latest date for each propertyCode\n",
    "latest_dates = union_dfs_idealista.groupBy(\"propertyCode\").agg(spark_max(\"date\").alias(\"latestDate\"))\n",
    "\n",
    "# Perform a left semi join to filter the union_dfs_idealista based on the latest dates\n",
    "latest_dfs_idealista = union_dfs_idealista.join(\n",
    "    latest_dates,\n",
    "    (union_dfs_idealista.propertyCode == latest_dates.propertyCode) & \n",
    "    (union_dfs_idealista.date == latest_dates.latestDate),\n",
    "    \"left_semi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_dfs_idealista.repartition(8, col(\"neighborhood\")).write.mode(\"overwrite\").parquet(\"../temporal_zone/idealista\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenData Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f'../P2_data/income_opendata/income_opendata_neighborhood.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"neigh_name \", \"neigh_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df = df.select(\"_id\", \"district_id\", \"neigh_name\", \"district_name\", F.explode(\"info\").alias(\"value_per_year\"))\n",
    "income_flattened = exploded_df.select(\"_id\", \"district_id\", \"neigh_name\", \"district_name\", col(\"value_per_year.RFD\").alias(\"rfd\"), col(\"value_per_year.pop\").alias(\"population\"), col(\"value_per_year.year\").alias(\"year\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_flattened.write.mode(\"overwrite\").parquet(\"../temporal_zone/neighborhood_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look-up table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_flattened = spark.read.parquet(\"../temporal_zone/neighborhood_data\")\n",
    "latest_dfs_idealista = spark.read.parquet(\"../temporal_zone/idealista\")\n",
    "\n",
    "lookup_ne_income = spark.read.json(\"../P2_data/lookup_tables/income_lookup_neighborhood.json\").select(\"neighborhood\", col(\"_id\").alias(\"id_neighorhood\"))\n",
    "lookup_dist_income = spark.read.json(\"../P2_data/lookup_tables/income_lookup_district.json\").select(\"district\", col(\"_id\").alias(\"id_district\"))\n",
    "\n",
    "lookup_ne_idealista = spark.read.json(\"../P2_data/lookup_tables/rent_lookup_neighborhood.json\").select(\"ne\", col(\"_id\").alias(\"id_neighorhood\"))\n",
    "lookup_dist_idealista = spark.read.json(\"../P2_data/lookup_tables/rent_lookup_district.json\").select(\"di\", col(\"_id\").alias(\"id_district\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_flattened = income_flattened.join(F.broadcast(lookup_ne_income), on=income_flattened['neigh_name'] == lookup_ne_income['neighborhood'])\n",
    "income_flattened = income_flattened.join(F.broadcast(lookup_dist_income), on=income_flattened['district_name'] == lookup_dist_income['district'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_dfs_idealista = latest_dfs_idealista.join(F.broadcast(lookup_ne_idealista), on=latest_dfs_idealista['neighborhood'] == lookup_ne_idealista['ne'])\n",
    "latest_dfs_idealista = latest_dfs_idealista.join(F.broadcast(lookup_dist_idealista), on=latest_dfs_idealista['district'] == lookup_dist_idealista['di'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_flattened.write.mode(\"overwrite\").parquet(\"../exploitation_zone/neighborhood_data\")\n",
    "latest_dfs_idealista.write.mode(\"overwrite\").parquet(\"../exploitation_zone/idealista\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
