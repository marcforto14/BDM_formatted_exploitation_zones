{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/31 11:20:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "    .builder \n",
    "    .appName(\"Preprocessing SDM\") \n",
    "    .config(\"spark.driver.memory\", \"2g\") \n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idealista Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Union Files in one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfs_idealista = [spark.read.parquet(f'../P2_data/idealista/{file}') for file in os.listdir('../P2_data/idealista')]\n",
    "\n",
    "\n",
    "# Convert each list to a set\n",
    "sets = [set(df.columns) for df in dfs_idealista]\n",
    "\n",
    "# Find the common elements among all sets\n",
    "common_values = set.intersection(*sets)\n",
    "\n",
    "all_variables = set.union(*sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neighborhood', 'newDevelopmentFinished', 'parkingSpace'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables - common_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def flatten_struct(df: DataFrame, struct_col: str) -> DataFrame:\n",
    "    struct_fields = df.schema[struct_col].dataType.fields\n",
    "    for field in struct_fields:\n",
    "        df = df.withColumn(f\"{struct_col}_{field.name}\", col(f\"{struct_col}.{field.name}\"))\n",
    "    df = df.drop(struct_col)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_dfs_idealista = []\n",
    "for df in dfs_idealista:\n",
    "    for columna , dtype in df.dtypes:\n",
    "        if \"struct\" in dtype:\n",
    "            df = (flatten_struct(df, columna))\n",
    "    flatten_dfs_idealista.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_dataframes_by_column(self, df2):\n",
    "    df1 = self\n",
    "    cols1 = set([*map(lambda x: str(x).lower(), df2.columns)]) - set(\n",
    "        [*map(lambda x: str(x).lower(), df1.columns)]\n",
    "    )\n",
    "    cols2 = set([*map(lambda x: str(x).lower(), df1.columns)]) - set(\n",
    "        [*map(lambda x: str(x).lower(), df2.columns)]\n",
    "    )\n",
    "\n",
    "    select_statement = []\n",
    "    for columna in cols1:\n",
    "        select_statement.append(\n",
    "            lit(None).alias(columna).cast(df2.select(columna).dtypes[0][1])\n",
    "        )\n",
    "\n",
    "    df1 = df1.select(*df1.columns, *select_statement)\n",
    "\n",
    "    select_statement = []\n",
    "    for columna in cols2:\n",
    "        select_statement.append(\n",
    "            lit(None).alias(columna).cast(df1.select(columna).dtypes[0][1])\n",
    "        )\n",
    "    df2 = df2.select(*df2.columns, *select_statement)\n",
    "\n",
    "    df1 = df1.select((df1.columns))\n",
    "    df2 = df2.select(\n",
    "        (df1.columns)\n",
    "    )  # ordenamos las columnas de la misma forma que el df1\n",
    "\n",
    "    df = df1.unionAll(df2)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dfs_idealista = flatten_dfs_idealista[0]\n",
    "for df in flatten_dfs_idealista[1:]:\n",
    "    union_dfs_idealista = union_dataframes_by_column(union_dfs_idealista, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle duplicates, reconciliate data, clean, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('address', 'string'),\n",
       " ('bathrooms', 'bigint'),\n",
       " ('country', 'string'),\n",
       " ('distance', 'string'),\n",
       " ('district', 'string'),\n",
       " ('exterior', 'boolean'),\n",
       " ('externalReference', 'string'),\n",
       " ('floor', 'string'),\n",
       " ('has360', 'boolean'),\n",
       " ('has3DTour', 'boolean'),\n",
       " ('hasLift', 'boolean'),\n",
       " ('hasPlan', 'boolean'),\n",
       " ('hasStaging', 'boolean'),\n",
       " ('hasVideo', 'boolean'),\n",
       " ('latitude', 'double'),\n",
       " ('longitude', 'double'),\n",
       " ('municipality', 'string'),\n",
       " ('neighborhood', 'string'),\n",
       " ('newDevelopment', 'boolean'),\n",
       " ('numPhotos', 'bigint'),\n",
       " ('operation', 'string'),\n",
       " ('price', 'double'),\n",
       " ('priceByArea', 'double'),\n",
       " ('propertyCode', 'string'),\n",
       " ('propertyType', 'string'),\n",
       " ('province', 'string'),\n",
       " ('rooms', 'bigint'),\n",
       " ('showAddress', 'boolean'),\n",
       " ('size', 'double'),\n",
       " ('status', 'string'),\n",
       " ('thumbnail', 'string'),\n",
       " ('topNewDevelopment', 'boolean'),\n",
       " ('url', 'string'),\n",
       " ('detailedType_subTypology', 'string'),\n",
       " ('detailedType_typology', 'string'),\n",
       " ('parkingSpace_hasParkingSpace', 'boolean'),\n",
       " ('parkingSpace_isParkingSpaceIncludedInPrice', 'boolean'),\n",
       " ('parkingSpace_parkingSpacePrice', 'double'),\n",
       " ('suggestedTexts_subtitle', 'string'),\n",
       " ('suggestedTexts_title', 'string'),\n",
       " ('newdevelopmentfinished', 'boolean')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union_dfs_idealista.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 129:================================================>    (117 + 4) / 129]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 21073 rows and 41 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def get_dataframe_shape(df):\n",
    "    num_rows = df.count()       # Count the number of rows\n",
    "    num_columns = len(df.columns)  # Get the number of columns\n",
    "    return num_rows, num_columns\n",
    "\n",
    "rows, cols = get_dataframe_shape(union_dfs_idealista)\n",
    "print(f\"The DataFrame has {rows} rows and {cols} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dfs_idealista = union_dfs_idealista.dropDuplicates(['propertyCode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 132:====================================================>(128 + 1) / 129]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 10421 rows and 41 columns.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rows, cols = get_dataframe_shape(union_dfs_idealista)\n",
    "print(f\"The DataFrame has {rows} rows and {cols} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_dfs_idealista = union_dfs_idealista.dropna(subset=['propertyCode', 'address', 'price', 'size'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenData Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(f'../P2_data/income_opendata/income_opendata_neighborhood.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"info_last\",(col(\"info\")[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[info_last: struct<RFD:double,pop:bigint,year:bigint>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"info_last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[NOT_COLUMN] Argument `col` should be a Column, got str.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpopulation_district\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfo_last.pop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrfd_district\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo_last.rfd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pyspark/sql/dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5127\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5128\u001b[0m \u001b[38;5;124;03mReturns a new :class:`DataFrame` by adding a column or replacing the\u001b[39;00m\n\u001b[1;32m   5129\u001b[0m \u001b[38;5;124;03mexisting column that has the same name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5167\u001b[0m \u001b[38;5;124;03m+---+-----+----+\u001b[39;00m\n\u001b[1;32m   5168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m-> 5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5173\u001b[0m     )\n\u001b[1;32m   5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mwithColumn(colName, col\u001b[38;5;241m.\u001b[39m_jc), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[0;31mPySparkTypeError\u001b[0m: [NOT_COLUMN] Argument `col` should be a Column, got str."
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"population_district\",col(\"info_last.pop\"))\n",
    "df = df.withColumn(\"rfd_district\",\"info_last.rfd\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
