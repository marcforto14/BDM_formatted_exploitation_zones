# ---
# jupyter:
#   jupytext:
#     formats: py,ipynb
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.5'
#       jupytext_version: 1.16.2
#   kernelspec:
#     display_name: base
#     language: python
#     name: python3
# ---

# +
from pyspark.sql import SparkSession
import os

from pyspark.sql import functions as F
from pyspark.sql import Window
from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler
from pyspark.ml.regression import GBTRegressor
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator

import seaborn as sns

# -

spark = (
    SparkSession.builder 
    .appName("Prediction BDM")
    .config("spark.executor.memory", "4g")
    .config("spark.driver.memory", "2g")
    .config("spark.sql.debug.maxToStringFields", 1000) 
    .getOrCreate()
)

# ## Joining dataframes

## Adding income data
income_data = spark.read.parquet("../exploitation_zone/neighborhood_data").filter(F.col("year") == 2017)
idealista_data = spark.read.parquet("../exploitation_zone/idealista")

income_data_dist = (
    income_data
    .groupBy("id_district")
    .agg(F.mean("rfd").alias("rfd_distr"), F.sum("population").alias("population_dist"))
)

idealista_data_w_income_data = (
    idealista_data
    .join(income_data.drop("id_district", "district", 'neighborhood'), on="id_neighorhood", how="left")
    .join(income_data_dist, on="id_district", how="left")
)

# ## Missing imputation

num = idealista_data_w_income_data.count()
select_statement = [
    (F.sum(F.col(columna).isNull().cast("integer"))/num).alias(f"{columna}_n_nulls")
    for columna in idealista_data_w_income_data.columns
]

idealista_data_w_income_data.select(*select_statement).show()

## We do not have some information on some neighorhood but we do have its information for the district, we will use that one. 
(
    idealista_data_w_income_data
    .filter(F.col("district_id").isNull())
    .dropDuplicates(["neighborhood"])
    .show(100, truncate=False)
)

idealista_data_w_income_data = idealista_data_w_income_data.withColumn("rfd", F.coalesce("rfd","rfd_distr"))
idealista_data_w_income_data = idealista_data_w_income_data.withColumn("population", F.coalesce("population","population_dist"))

## We will impute the floor by the most probable in a given neighorhood
w = Window.partitionBy("id_neighorhood")
idealista_data_w_income_data = idealista_data_w_income_data.withColumn("floor", F.when(F.col("floor").isNull(), F.mode("floor").over(w)).otherwise(F.col("floor")))

# ## Drop Unused columns

# Drop unused info
idealista_data_wo_unused_cols = idealista_data_w_income_data.drop("ne", "di", "_id", "district_id", "neigh_name", "district_name", "year", "rfd_distr", "population_dist", "propertyCode", "neighborhood")

# +
df = idealista_data_wo_unused_cols 

# Define categorical and numeric features
categorical_features = ["id_district", "id_neighorhood", "status", "propertyType", "district", "floor"]
numeric_cols = ["bathrooms", "rooms", "latitude", "size", "longitude", "rfd", "population"]

# StringIndexer for categorical columns
indexers = [StringIndexer(inputCol=column, outputCol=column+"_index").fit(df) for column in categorical_features]

# Apply indexers
for indexer in indexers:
    df = indexer.transform(df)

# OneHotEncoder for indexed columns
encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers], 
                        outputCols=[column+"_ohe" for column in categorical_features])
df = encoder.fit(df).transform(df)

# Assemble numeric columns into a vector
assembler = VectorAssembler(inputCols=numeric_cols, outputCol="numeric_features")
df = assembler.transform(df)

# Scale the numeric features
scaler = StandardScaler(inputCol="numeric_features", outputCol="scaled_numeric_features")
df = scaler.fit(df).transform(df)

# Combine one-hot encoded columns and scaled numeric features into a single feature vector
feature_cols = ["scaled_numeric_features"] + [column+"_ohe" for column in categorical_features]
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
df = assembler.transform(df)

# Select features and price
final_df = df.select("features", F.col("price").alias("label"))
# -

# Split the data into training and test sets
train_data, test_data = final_df.randomSplit([0.8, 0.2], seed=1234)

# +
train_data.write.mode("overwrite").parquet("../temporal_zone/train_data")
train_data = spark.read.parquet("../temporal_zone/train_data")

test_data.write.mode("overwrite").parquet("../temporal_zone/test_data")
test_data = spark.read.parquet("../temporal_zone/test_data")
# -

# Define the GBT Regressor
gbt = GBTRegressor(featuresCol="features", labelCol="label", maxIter=100)

# Create a pipeline with the GBT model
pipeline = Pipeline(stages=[gbt])


# Train the model
gbt_model = pipeline.fit(train_data)

# Save the trained model
model_save_path = "../model/gbt_model"
gbt_model.write().overwrite().save(model_save_path)

# Make predictions
predictions = gbt_model.transform(test_data)
actual_vs_expected = predictions.select("prediction", "label")


# Evaluate the model
evaluator = RegressionEvaluator(labelCol="label", predictionCol="prediction", metricName="mae")
mae = evaluator.evaluate(predictions)
print(f"Mean absolute error on test data = {mae}")


actual_vs_expected = actual_vs_expected.withColumn("loss", F.abs(F.col("prediction") - F.col("label")))
actual_vs_expected = actual_vs_expected.withColumn("prc_loss", F.abs(F.col("loss")/F.col("label")))


## As the data is fairly small only 4 columns we can pass is to pandas but this won't scale!
viz_ave = actual_vs_expected.toPandas()

## Distribution of % loss.
sns.histplot(data=viz_ave, x='prc_loss')

## Biggest errors are in very low priced houses or very high
sns.scatterplot(data=viz_ave, x='label', y='prc_loss')
